{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Tutorial Using OpenAI Gym toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I - Gym toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen Lake Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-18 16:47:06,890] Making new env: FrozenLake8x8-v0\n"
     ]
    }
   ],
   "source": [
    "lake = gym.make('FrozenLake8x8-v0') #loading the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "lake.reset() #reset the environment to the initial state\n",
    "lake.render() #render one frame of the environmnet for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action sapece: Discrete(4)\n",
      "state space: Discrete(64)\n"
     ]
    }
   ],
   "source": [
    "print('action sapece:',lake.action_space) #LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3\n",
    "print('state space:',lake.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually assign a state with `env.env.s` and render one frame to see the result, current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "lake.env.s = 8\n",
    "lake.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters P, nA, nS and gamma defined as:\n",
    "* $nS$ `int`: number of states in the environment \n",
    "* $nA$ `int`: number of actions in the environmnet\n",
    "* $\\gamma$ `float`: discount factor in range $[0,1)$\n",
    "* $P$: nested dictionary from `gym.core.Environment` where for each pair of states in $[1,nS]$ and actions in $[1, nA]$, $P[state][action]$ is a tuple of the form *(Probability, nextstate, reward, terminal)* where:\n",
    "    * *probability* `float` is the probably of transitioning from *state* to *nextstate*\n",
    "    * *nextstate* `int` denotes the state we transition to\n",
    "    * *reward* `int` is either $0$ or $1$ the reward for transitioning from *state* to *nextstate* with *action*\n",
    "    * *terminal* `bool` is $True$ when *nextstate* is a terminal state *(H,G)* and $False$ otherwise\n",
    "    \n",
    "``` P = {s : {a : [] for a in range(nA)} for s in range(nS)} ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 8, 0.0, False),\n",
       "  (0.3333333333333333, 16, 0.0, False)],\n",
       " 1: [(0.3333333333333333, 8, 0.0, False),\n",
       "  (0.3333333333333333, 16, 0.0, False),\n",
       "  (0.3333333333333333, 9, 0.0, False)],\n",
       " 2: [(0.3333333333333333, 16, 0.0, False),\n",
       "  (0.3333333333333333, 9, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False)],\n",
       " 3: [(0.3333333333333333, 9, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 8, 0.0, False)]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lake.env.P[8] #shows P[8][.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 0.0, False, {'prob': 0.3333333333333333})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Step the environment by one timestep. \n",
    "Returns observation: Observations, reward, done, info\n",
    "'''\n",
    "lake.step(1) #take action \"down\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 0.0, False, {'prob': 0.3333333333333333})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lake.step(lake.action_space.sample()) #take one random action of all possible actions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here is the Demonstration of a single episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 22\n",
      "Total reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "lake.reset()\n",
    "epochs = 0\n",
    "done = False\n",
    "frames = []\n",
    "\n",
    "while not done:\n",
    "    action = lake.action_space.sample()\n",
    "    state, reward, done, info = lake.step(action)\n",
    "    frames.append({\n",
    "        'frame': lake.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        })\n",
    "    epochs += 1\n",
    "    \n",
    "print(\"Timesteps taken:\", epochs)\n",
    "print('Total reward:', reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFF\u001b[41mH\u001b[0mFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "Timestep: 22\n",
      "State: 35\n",
      "Action: 3\n",
      "Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'].getvalue())\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a demonstration of reaching the goal following the default stochastic policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of episodes: 620\n",
      "Timesteps taken: 41\n",
      "Total reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "goal = 0\n",
    "while goal==0:\n",
    "    lake.reset()\n",
    "    epochs = 0\n",
    "    done = False\n",
    "    frames = []\n",
    "    while not done:\n",
    "        action = lake.action_space.sample()\n",
    "        state, reward, done, info = lake.step(action)\n",
    "        goal =+ reward\n",
    "        frames.append({\n",
    "            'frame': lake.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "            })\n",
    "        epochs += 1\n",
    "    iteration += 1\n",
    "\n",
    "print('Total number of episodes:', iteration)\n",
    "print(\"Timesteps taken:\", epochs)\n",
    "print('Total reward:', goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n",
      "\n",
      "Timestep: 41\n",
      "State: 63\n",
      "Action: 1\n",
      "Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As demonstrated above, agent takes thousands of time steps to reach the goal once. Because the policy is not optimal (take random action at each time step) and agent is not learning how to improve its policy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
